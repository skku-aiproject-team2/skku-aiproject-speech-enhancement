{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Subset\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import time\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "from scipy.io.wavfile import write as wavwrite\n",
    "from scipy.io.wavfile import read as wavread\n",
    "\n",
    "from dataset import load_CleanNoisyPairDataset\n",
    "from util import rescale, find_max_epoch, print_size, sampling\n",
    "from network import CleanUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(output_directory, ckpt_iter, subset, num, gpu, opt, dump=False):\n",
    "    \"\"\"\n",
    "    Denoise audio\n",
    "\n",
    "    Parameters:\n",
    "    output_directory (str):         save generated speeches to this path\n",
    "    ckpt_iter (int or 'max'):       the pretrained checkpoint to be loaded; \n",
    "                                    automitically selects the maximum iteration if 'max' is selected\n",
    "    subset (str):                   training, testing, validation\n",
    "    num (int):                      number of samples to use in inference, use all if 0.\n",
    "    gpu (bool):                     whether to run on gpu\n",
    "    opt (bool):                     wheter to use optimazation scheme\n",
    "    dump (bool):                    whether save enhanced (denoised) audio\n",
    "    \"\"\"\n",
    "\n",
    "    # setup local experiment path\n",
    "    exp_path = train_config[\"exp_path\"]\n",
    "    print('exp_path:', exp_path)\n",
    "\n",
    "    # load data\n",
    "    loader_config = deepcopy(trainset_config)\n",
    "    loader_config[\"crop_length_sec\"] = 0\n",
    "    dataloader = load_CleanNoisyPairDataset(\n",
    "        **loader_config, \n",
    "        subset=subset,\n",
    "        batch_size=1, \n",
    "        num_gpus=1\n",
    "    )\n",
    "    if num == 0:\n",
    "        num = len(dataloader)\n",
    "\n",
    "    # predefine model\n",
    "    device = 'cuda' if gpu else 'cpu'\n",
    "    if(gpu):\n",
    "        assert torch.cuda.is_available()\n",
    "    net = CleanUNet(**network_config, **opt_config).to(device)\n",
    "    print_size(net)\n",
    "\n",
    "    # load checkpoint\n",
    "    ckpt_directory = os.path.join(train_config[\"log\"][\"directory\"], exp_path, 'checkpoint')\n",
    "    if ckpt_iter == 'max':\n",
    "        ckpt_iter = find_max_epoch(ckpt_directory)\n",
    "    if ckpt_iter != 'pretrained':\n",
    "        ckpt_iter = int(ckpt_iter)\n",
    "    model_path = os.path.join(ckpt_directory, '{}.pkl'.format(ckpt_iter))\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    net.eval()\n",
    "\n",
    "    # get output directory ready\n",
    "    if ckpt_iter == \"pretrained\":\n",
    "        speech_directory = os.path.join(output_directory, exp_path, 'speech', ckpt_iter) \n",
    "    else:\n",
    "        speech_directory = os.path.join(output_directory, exp_path, 'speech', '{}k'.format(ckpt_iter//1000))\n",
    "    if dump and not os.path.isdir(speech_directory):\n",
    "        os.makedirs(speech_directory)\n",
    "        os.chmod(speech_directory, 0o775)\n",
    "    print(\"speech_directory: \", speech_directory, flush=True)\n",
    "\n",
    "    # inference\n",
    "    all_generated_audio = []\n",
    "    all_clean_audio = []\n",
    "    sortkey = lambda name: '_'.join(name.split('/')[-1].split('_')[1:])\n",
    "\n",
    "    avg_time = 0\n",
    "    iter = 1\n",
    "    with tqdm(total = num, disable =True) as pbar:\n",
    "        for clean_audio, noisy_audio, fileid in dataloader:\n",
    "            # if not gpu:\n",
    "                # clean_audio, noisy_audio = clean_audio.to('cpu'), noisy_audio.to('cpu')\n",
    "            # else:\n",
    "                # noisy_audio = noisy_audio.cuda()\n",
    "            clean_audio, noisy_audio = clean_audio.to(device), noisy_audio.to(device)\n",
    "\n",
    "            filename = sortkey(fileid[0][0])\n",
    "            print(\"input shape\", noisy_audio.shape)\n",
    "            LENGTH = len(noisy_audio[0].squeeze())\n",
    "            start_time = time.time()\n",
    "            generated_audio = sampling(net, noisy_audio)\n",
    "            \n",
    "            if dump:\n",
    "                wavwrite(os.path.join(speech_directory, 'enhanced_{}'.format(filename)), \n",
    "                        trainset_config[\"sample_rate\"],\n",
    "                        generated_audio[0].squeeze().cpu().numpy())\n",
    "            else:\n",
    "                all_clean_audio.append(clean_audio[0].squeeze().cpu().numpy())\n",
    "                all_generated_audio.append(generated_audio[0].squeeze().cpu().numpy())\n",
    "                \n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "            avg_time += elapsed_time\n",
    "            pbar.set_postfix({\"Average Time\": f\"{avg_time / iter:.6f}\"})\n",
    "            pbar.update(1)\n",
    "  \n",
    "            \n",
    "            if iter == num:\n",
    "                break\n",
    "            iter+=1\n",
    "\n",
    "    print(\"Average time: \", avg_time / iter)\n",
    "    return all_clean_audio, all_generated_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_config = \"configs/valentini.json\"\n",
    "ckpt_iter = 85000\n",
    "subset = \"testing\"\n",
    "num = 1\n",
    "gpu = False\n",
    "opt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(_config) as f:\n",
    "    data = f.read()\n",
    "config = json.loads(data)\n",
    "gen_config              = config[\"gen_config\"]\n",
    "global network_config\n",
    "network_config          = config[\"network_config\"]      # to define wavenet\n",
    "global train_config\n",
    "train_config            = config[\"train_config\"]        # train config\n",
    "global trainset_config\n",
    "trainset_config         = config[\"trainset_config\"]     # to read trainset configurations\n",
    "if opt==True:\n",
    "    global opt_config\n",
    "    opt_config         = config[\"opt_config\"] \n",
    "else:\n",
    "    opt_config          = {}\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_path: valentini\n",
      "CleanUNet Parameters: 46.071937M;  \n",
      "speech_directory:  ./exp/valentini/speech/85k\n",
      "input shape torch.Size([1, 1, 69174])\n",
      "encoder start:  torch.Size([1, 1, 69374])\n",
      "torch.Size([1, 64, 34686])\n",
      "torch.Size([1, 128, 17342])\n",
      "torch.Size([1, 256, 8670])\n",
      "torch.Size([1, 512, 4334])\n",
      "torch.Size([1, 768, 2166])\n",
      "torch.Size([1, 768, 1082])\n",
      "torch.Size([1, 768, 540])\n",
      "torch.Size([1, 768, 269])\n",
      "decoder start:  torch.Size([1, 768, 269])\n",
      "torch.Size([1, 768, 540])\n",
      "torch.Size([1, 768, 1082])\n",
      "torch.Size([1, 768, 2166])\n",
      "torch.Size([1, 512, 4334])\n",
      "torch.Size([1, 256, 8670])\n",
      "torch.Size([1, 128, 17342])\n",
      "torch.Size([1, 64, 34686])\n",
      "torch.Size([1, 1, 69374])\n",
      "Average time:  1.0802733898162842\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    denoise(gen_config[\"output_directory\"],\n",
    "            subset=subset,\n",
    "            ckpt_iter=ckpt_iter,\n",
    "            num=num,\n",
    "            gpu=gpu,\n",
    "            opt=opt,\n",
    "            dump=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768, 574])\n",
      "torch.Size([2, 768, 1150])\n",
      "torch.Size([2, 768, 1150])\n",
      "torch.Size([2, 768, 2302])\n",
      "torch.Size([2, 768, 2302])\n",
      "torch.Size([2, 768, 4606])\n",
      "torch.Size([2, 768, 4606])\n",
      "torch.Size([2, 512, 9214])\n",
      "torch.Size([2, 512, 9214])\n",
      "torch.Size([2, 256, 18430])\n",
      "torch.Size([2, 256, 18430])\n",
      "torch.Size([2, 128, 36862])\n",
      "torch.Size([2, 128, 36862])\n",
      "torch.Size([2, 64, 73726])\n",
      "torch.Size([2, 64, 73726])\n",
      "torch.Size([2, 1, 147454])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1            [-1, 64, 73726]             320\n",
      "              ReLU-2            [-1, 64, 73726]               0\n",
      "            Conv1d-3           [-1, 128, 73726]           8,320\n",
      "               GLU-4            [-1, 64, 73726]               0\n",
      "            Conv1d-5           [-1, 128, 36862]          32,896\n",
      "              ReLU-6           [-1, 128, 36862]               0\n",
      "            Conv1d-7           [-1, 256, 36862]          33,024\n",
      "               GLU-8           [-1, 128, 36862]               0\n",
      "            Conv1d-9           [-1, 256, 18430]         131,328\n",
      "             ReLU-10           [-1, 256, 18430]               0\n",
      "           Conv1d-11           [-1, 512, 18430]         131,584\n",
      "              GLU-12           [-1, 256, 18430]               0\n",
      "           Conv1d-13            [-1, 512, 9214]         524,800\n",
      "             ReLU-14            [-1, 512, 9214]               0\n",
      "           Conv1d-15           [-1, 1024, 9214]         525,312\n",
      "              GLU-16            [-1, 512, 9214]               0\n",
      "           Conv1d-17            [-1, 768, 4606]       1,573,632\n",
      "             ReLU-18            [-1, 768, 4606]               0\n",
      "           Conv1d-19           [-1, 1536, 4606]       1,181,184\n",
      "              GLU-20            [-1, 768, 4606]               0\n",
      "           Conv1d-21            [-1, 768, 2302]       2,360,064\n",
      "             ReLU-22            [-1, 768, 2302]               0\n",
      "           Conv1d-23           [-1, 1536, 2302]       1,181,184\n",
      "              GLU-24            [-1, 768, 2302]               0\n",
      "           Conv1d-25            [-1, 768, 1150]       2,360,064\n",
      "             ReLU-26            [-1, 768, 1150]               0\n",
      "           Conv1d-27           [-1, 1536, 1150]       1,181,184\n",
      "              GLU-28            [-1, 768, 1150]               0\n",
      "           Conv1d-29             [-1, 768, 574]       2,360,064\n",
      "             ReLU-30             [-1, 768, 574]               0\n",
      "           Conv1d-31            [-1, 1536, 574]       1,181,184\n",
      "              GLU-32             [-1, 768, 574]               0\n",
      "           Conv1d-33             [-1, 512, 574]         393,728\n",
      "          Dropout-34             [-1, 574, 512]               0\n",
      "        LayerNorm-35             [-1, 574, 512]           1,024\n",
      "           Linear-36             [-1, 574, 512]         262,144\n",
      "           Linear-37             [-1, 574, 512]         262,144\n",
      "           Linear-38             [-1, 574, 512]         262,144\n",
      "          Dropout-39          [-1, 8, 574, 574]               0\n",
      "ScaledDotProductAttention-40  [[-1, 8, 574, 64], [-1, 8, 574, 574]]               0\n",
      "           Linear-41             [-1, 574, 512]         262,144\n",
      "          Dropout-42             [-1, 574, 512]               0\n",
      "        LayerNorm-43             [-1, 574, 512]           1,024\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[38;5;241m=\u001b[39m CleanUNet(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnetwork_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopt_config)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m147266\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aip/lib/python3.12/site-packages/torchsummary/torchsummary.py:93\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     87\u001b[0m line_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:>20}\u001b[39;00m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{:>25}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{:>15}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     88\u001b[0m     layer,\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28mstr\u001b[39m(summary[layer][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0:,}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(summary[layer][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnb_params\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     92\u001b[0m total_params \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m summary[layer][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnb_params\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 93\u001b[0m total_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainable\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m summary[layer]:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m summary[layer][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/aip/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3100\u001b[0m, in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2979\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_prod_dispatcher)\n\u001b[1;32m   2980\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprod\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2981\u001b[0m          initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2982\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2983\u001b[0m \u001b[38;5;124;03m    Return the product of array elements over a given axis.\u001b[39;00m\n\u001b[1;32m   2984\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3098\u001b[0m \u001b[38;5;124;03m    10\u001b[39;00m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3101\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aip/lib/python3.12/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "net = CleanUNet(**network_config, **opt_config).to('cuda')\n",
    "from torchsummary import summary\n",
    "print(summary(net,input_size = (1,147266)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 48126])\n",
      "torch.Size([1, 64, 24062])\n",
      "torch.Size([1, 64, 24062])\n",
      "torch.Size([1, 128, 24062])\n",
      "torch.Size([1, 64, 24062])\n",
      "decoder\n",
      "torch.Size([1, 64, 24062])\n",
      "torch.Size([1, 128, 24062])\n",
      "torch.Size([1, 64, 24062])\n",
      "torch.Size([1, 1, 48126])\n",
      "decoder1\n",
      "torch.Size([1, 64, 24062])\n",
      "torch.Size([1, 128, 24064])\n",
      "torch.Size([1, 64, 24064])\n",
      "torch.Size([1, 64, 48128])\n",
      "torch.Size([1, 16, 48128])\n",
      "torch.Size([1, 1, 48128])\n",
      "torch.Size([1, 1, 48128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "channels_input=1\n",
    "channels_output=1\n",
    "channels_H=64\n",
    "max_H=768\n",
    "encoder_n_layers=8\n",
    "kernel_size=4\n",
    "stride=2\n",
    "\n",
    "class AverageChannels(nn.Module):\n",
    "    def __init__(self, factor):\n",
    "        super(AverageChannels, self).__init__()\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, length = x.size()\n",
    "        # Reshape the input to (batch_size, channels // factor, factor, length)\n",
    "        x = x.view(batch_size, channels // self.factor, self.factor, length)\n",
    "        # Average over the factor dimension\n",
    "        x = x.mean(dim=2)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "x = torch.randn(1,1,48126)\n",
    "enc = [nn.Conv1d(channels_input, channels_H, kernel_size, stride),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv1d(channels_H, channels_H * 2, 1), \n",
    "                nn.GLU(dim=1)]\n",
    "# channels_H = channels_H//2\n",
    "dec = [nn.Conv1d(channels_H, channels_H * 2, 1), \n",
    "                        nn.GLU(dim=1),\n",
    "                        nn.ConvTranspose1d(channels_H, channels_output, kernel_size, stride)]\n",
    "dec1 = [nn.Conv1d(channels_H, channels_H * 2, 1, padding=1), \n",
    "                        nn.GLU(dim=1),\n",
    "                        nn.Upsample(scale_factor=stride, mode='linear', align_corners=False),\n",
    "                        AverageChannels(factor=4),\n",
    "                        nn.Conv1d(channels_H//4, channels_output, kernel_size,stride = 1, padding='same'),\n",
    "                        nn.ReLU()]\n",
    "print(x.shape)\n",
    "for e in enc:\n",
    "    x = e(x)\n",
    "    print(x.shape)\n",
    "x = torch.randn(1,64,24062)\n",
    "print(\"decoder\")\n",
    "print(x.shape)\n",
    "for d in dec:\n",
    "    x = d(x)\n",
    "    print(x.shape)\n",
    "\n",
    "x = torch.randn(1,64,24062)\n",
    "print(\"decoder1\")\n",
    "print(x.shape)\n",
    "for d in dec1:\n",
    "    x = d(x)\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
