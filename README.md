## Purpose of this project

### online lectures in Sungkyunkwan University
Many courses at Sungkyunkwan University are currently conducted through recorded online lectures, and even for offline classes, recorded lectures are often provided. However, most of these recordings are done in general classrooms or offices without proper audio equipment or noise control, rather than in a professional studio environment. This leads to the intrusion of student or external noise during lectures, resulting in poor audio quality, which in turn makes it difficult for students to follow the lectures and maintain concentration. This issue is particularly problematic for major courses conducted in English, where students who are not familiar with English speakers or technical terms may find it even harder to recognize pronunciations when noise is present. 

### Our goal : cpu inference
We have selected CleanUNet(https://github.com/NVIDIA/CleanUNet) as the baseline model for audio enhancement. Our goal is to use this model to improve the audio quality and subsequently measure the improvement in audio clarity. Furthermore, we aim to enhance the performance of the baseline model and compare the results with the original CleanUNet baseline.


## Dataset Description

We utilized two datasets in the project. The first is Valentini dataset which includes audio with no noise and the corresponding audio with synthetic audio merged. Valentini dataset was used for training and test to measure performance of the trained model. The latter dataset consists fo two audio files that range about 10 mins that are extracted from Youtube. This data was used to simulate how well our model works in real-time to enhance audio quality of the lectures in Sungkyunkwan University. 

### Overview of Valentini Dataset

**Description**: Clean and noisy parallel speech database. The database was designed to train and test speech enhancement methods that operate at 48kHz. It was created by Christian Valentini-Botinhao and is widely used in research and development of speech enhancement algorithms.

### Dataset Composition

The Valentini dataset consists of clean and noisy speech recordings, which are used to train and evaluate noise suppression models. The dataset includes the following components:

#### Clean Speech
- The clean speech recordings are sourced from the VoiceBank corpus.
- The corpus includes recordings from 28 speakers (14 male and 14 female), providing a diverse set of speech samples.
- The clean speech files are typically in the .wav format with a sampling rate of 48 kHz.

#### Noisy Speech
- The noisy speech recordings are generated by mixing the clean speech with various noise types at different signal-to-noise ratios (SNRs).
- Noise types include different environmental sounds like speech babble, destroyer engine, and a wide range of everyday background noises.
- The noisy speech files are also in the .wav format, usually downsampled to 16 kHz for consistency and ease of processing.

#### Data Preparation
The dataset is split into training and testing sets. Clean and noisy speech pairs are prepared for model training.

#### Model Training
Models like CleanUnet or other deep learning architectures are trained using the prepared dataset. The model learns to map noisy speech to clean speech.

#### Model Evaluation
The trained model is evaluated on a separate test set from the Valentini dataset. Performance metrics help determine how well the model suppresses noise and improves speech quality.

#### Overfitting Evaluation
The trained model's overfitting is evaluated on a separate test set named 'valid'. 

### Actual Dataset

- `clean_trainset_wav`: 23,075 files
- `noisy_trainset_wav`: 23,075 files
- `clean_testset_wav`: 824 files
- `noisy_testset_wav`: 824 files

#### Used for Training

- `train/clean`: 23,075 files
- `train/noisy`: 23,075 files

#### Used for Evaluation

- `test/clean`: 412 files
- `test/noisy`: 412 files

#### Used for Overfitting Check

- `valid/clean`: 412 files
- `valid/noisy`: 412 files

#### Data Handling and Sampling

When training, a random crop of 3 seconds is taken from the audio. To check for overfitting, a separate validation set was created from the test set. Download the dataset and place it in the `dataset/` folder, then pull the latest commits from GitHub related to the validation loader to start training.

#### Handling Short Audio Clips

To address short audio clips (around 1 second) in the Valentini dataset, adjustments were made to ensure consistent processing. Both train and test datasets contain 4-second .wav files.

## Training

The ```$EXP``` variable can be any config name in ```./configs/```, such as ```DNS-large-full``` and ```DNS-large-high```. The default experiment path is ```./exp```; it can be changed by modifying ```train_config[log[directory]]``` in the config files. ```trainset_config[root]``` needs to be set as the root path of the dataset. Then, the training code is

```python3 distributed.py -c configs/${EXP}.json```

We use 8 GPUs for training. The global batch size is 64 and we train the models for 250K iterations. Note that, this is different from the training setup in our paper i.e., 1M iterations with a batch size of 16. We find negligible difference in terms of objective and subjective evaluation, but the current setup is faster.

**Pre-trained** models for denoising are provided in ```./exp/${EXP}/checkpoint/pretrained.pkl``` (each one has size ~177Mb; use ```git lfs``` to download). Note that these models are not trained to remove reverb. 

## Denoising

We perform denoising on the DNS no-reverb test dataset. The output path is ```gen_config[output_directory]```, which is ```./exp``` by default. The denoising code is

```python denoise.py -c configs/${EXP}.json --ckpt_iter ${ITERATION}```

For example, if you want to use pre-trained models to denoise, run:

```python denoise.py -c configs/DNS-large-high.json --ckpt_iter pretrained```

1 GPU is used for denoising.

## Evaluation

The following evaluation code generates [PESQ](https://www.itu.int/rec/T-REC-P.862) and [STOI](https://ceestaal.nl/code/) scores. More evaluation metrics can be found in the [SEGAN (PyTorch)](https://github.com/santi-pdp/segan_pytorch) repo.

```python python_eval.py -d dns -e ${PATH_TO_DENOISED_SPEECH} -t ${PATH_TO_TESTSET_PATH} >> eval.log```

1 GPU is used for evaluation.

## Requirements

To synthesize [Microsoft DNS 2020](https://arxiv.org/ftp/arxiv/papers/2005/2005.13981.pdf) training data, you need [these dependencies](https://github.com/microsoft/DNS-Challenge/blob/interspeech2020/master/requirements.txt). If you just want to evaluate our pre-trained models on the test data, you may jump this.

Our code is tested on 8 NVIDIA V100 GPUs. You need to install very standard dependencies: ```numpy``` and ```scipy``` for scientific computing, ```torch, torchvision, torchaudio``` for deep learning and data loading, ```pesq, pystoi``` for audio evaluation, and ```tqdm``` for visualization.

## References

The code structure and distributed training are adapted from [WaveGlow (PyTorch)](https://github.com/NVIDIA/waveglow) (BSD-3-Clause license). The ```stft_loss.py``` is adapted from [ParallelWaveGAN (PyTorch)](https://github.com/kan-bayashi/ParallelWaveGAN) (MIT license). The self-attention blocks in ```network.py``` is adapted from [Attention is all you need (PyTorch)](https://github.com/jadore801120/attention-is-all-you-need-pytorch) (MIT license), which borrows from [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py) (MIT license). The learning rate scheduler in ```util.py``` is adapted from [VQVAE2 (PyTorch)](https://github.com/rosinality/vq-vae-2-pytorch) (MIT license). Some utility functions are borrowed from [DiffWave (PyTorch)](https://github.com/philsyn/DiffWave-Vocoder) (MIT license) and [WaveGlow (PyTorch)](https://github.com/NVIDIA/waveglow) (BSD-3-Clause license).

For more evaluation methods, we refer readers to look at [SEGAN (PyTorch)](https://github.com/santi-pdp/segan_pytorch/blob/master/segan/utils.py) (MIT license). For more data augmentation methods, we refer readers to look at [FAIR-denoiser](https://github.com/facebookresearch/denoiser/blob/main/denoiser/augment.py) (CC-BY-NC 4.0 license). 

## Citation

```
@inproceedings{kong2022speech,
  title={Speech Denoising in the Waveform Domain with Self-Attention},
  author={Kong, Zhifeng and Ping, Wei and Dantrey, Ambrish and Catanzaro, Bryan},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7867--7871},
  year={2022},
  organization={IEEE}
}
```
